# DeepSeekæœ¬åœ°éƒ¨ç½²ä¸å¾®è°ƒè®­ç»ƒæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•ï¼š
1. åœ¨æœ¬åœ°éƒ¨ç½²DeepSeekå¼€æºæ¨¡å‹
2. å¯¹DeepSeekæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆFine-tuningï¼‰ä»¥é€‚åº”äº¤æ˜“åœºæ™¯
3. é›†æˆåˆ°AIæ¨ç†æœåŠ¡ä¸­

---

## ğŸ¯ æ–¹æ¡ˆé€‰æ‹©

### æ–¹æ¡ˆ1ï¼šä½¿ç”¨DeepSeek-Coderï¼ˆæ¨èç”¨äºäº¤æ˜“åˆ†æï¼‰

DeepSeek-Coderæ˜¯DeepSeekä¸“é—¨ä¸ºä»£ç å’Œé€»è¾‘æ¨ç†è®­ç»ƒçš„æ¨¡å‹ï¼Œé€‚åˆäº¤æ˜“ä¿¡å·åˆ†æã€‚

**ä¼˜åŠ¿**ï¼š
- âœ… é€»è¾‘æ¨ç†èƒ½åŠ›å¼º
- âœ… æ”¯æŒä»£ç ç”Ÿæˆï¼ˆå¯ç”¨äºç­–ç•¥å›æµ‹ï¼‰
- âœ… å¼€æºï¼Œå¯æœ¬åœ°éƒ¨ç½²
- âœ… æ”¯æŒå¾®è°ƒ

**æ¨¡å‹å¤§å°**ï¼š
- DeepSeek-Coder-1.3B: ~2.6GBï¼ˆé€‚åˆ16GBæ˜¾å­˜ï¼‰
- DeepSeek-Coder-6.7B: ~13GBï¼ˆé€‚åˆ24GBæ˜¾å­˜ï¼‰
- DeepSeek-Coder-33B: ~66GBï¼ˆéœ€è¦å¤šå¡ï¼‰

### æ–¹æ¡ˆ2ï¼šä½¿ç”¨DeepSeek-Chatï¼ˆé€šç”¨å¯¹è¯ï¼‰

DeepSeek-Chatæ˜¯é€šç”¨å¯¹è¯æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ç”¨äºäº¤æ˜“åˆ†æã€‚

**æ¨¡å‹å¤§å°**ï¼š
- DeepSeek-Chat-1.3B: ~2.6GB
- DeepSeek-Chat-6.7B: ~13GB

---

## ğŸ“¦ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

### 1.1 ç¡¬ä»¶è¦æ±‚

**æœ€ä½é…ç½®**ï¼š
- GPU: NVIDIA GPU with 16GB+ VRAMï¼ˆå¦‚RTX 4090, A100ï¼‰
- RAM: 32GB+
- å­˜å‚¨: 50GB+ å¯ç”¨ç©ºé—´

**æ¨èé…ç½®**ï¼š
- GPU: NVIDIA A100 40GBæˆ–æ›´é«˜
- RAM: 64GB+
- å­˜å‚¨: 100GB+ SSD

### 1.2 è½¯ä»¶å®‰è£…

```bash
# å®‰è£…CUDA Toolkitï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
# æ£€æŸ¥CUDAç‰ˆæœ¬
nvidia-smi

# å®‰è£…Python 3.10+
python3 --version

# å®‰è£…PyTorchï¼ˆæ ¹æ®CUDAç‰ˆæœ¬ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…vLLMï¼ˆç”¨äºé«˜æ•ˆæ¨ç†ï¼‰
pip install vllm

# æˆ–ä½¿ç”¨Transformersï¼ˆç”¨äºå¾®è°ƒï¼‰
pip install transformers accelerate datasets peft

# å®‰è£…DeepSeekç›¸å…³ä¾èµ–
pip install deepseek-ai
```

---

## ğŸš€ ç¬¬äºŒæ­¥ï¼šæœ¬åœ°éƒ¨ç½²DeepSeekæ¨¡å‹

### 2.1 ä¸‹è½½æ¨¡å‹

```bash
# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p ai-inference-service/models/deepseek
cd ai-inference-service/models/deepseek

# ä½¿ç”¨HuggingFace CLIä¸‹è½½æ¨¡å‹ï¼ˆéœ€è¦å…ˆç™»å½•ï¼‰
huggingface-cli login

# ä¸‹è½½DeepSeek-Coder-6.7Bï¼ˆæ¨èï¼‰
huggingface-cli download deepseek-ai/deepseek-coder-6.7b-instruct \
    --local-dir ./deepseek-coder-6.7b \
    --local-dir-use-symlinks False

# æˆ–è€…ä¸‹è½½DeepSeek-Chat-6.7B
huggingface-cli download deepseek-ai/deepseek-chat-6.7b \
    --local-dir ./deepseek-chat-6.7b \
    --local-dir-use-symlinks False
```

### 2.2 ä½¿ç”¨vLLMéƒ¨ç½²ï¼ˆæ¨èï¼Œé«˜æ€§èƒ½ï¼‰

åˆ›å»ºéƒ¨ç½²è„šæœ¬ `ai-inference-service/scripts/deploy_deepseek_local.py`ï¼š

```python
"""
DeepSeekæœ¬åœ°éƒ¨ç½²è„šæœ¬
ä½¿ç”¨vLLMè¿›è¡Œé«˜æ•ˆæ¨ç†
"""
import argparse
from vllm import LLM, SamplingParams

def deploy_deepseek(
    model_path: str = "./models/deepseek/deepseek-coder-6.7b",
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = 0.9
):
    """
    éƒ¨ç½²DeepSeekæ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        tensor_parallel_size: å¹¶è¡ŒGPUæ•°é‡
        gpu_memory_utilization: GPUå†…å­˜ä½¿ç”¨ç‡
    """
    print(f"ğŸš€ åŠ è½½DeepSeekæ¨¡å‹: {model_path}")
    
    # åˆå§‹åŒ–LLM
    llm = LLM(
        model=model_path,
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=gpu_memory_utilization,
        trust_remote_code=True
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    # æµ‹è¯•æ¨ç†
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.95,
        max_tokens=512
    )
    
    prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“AIåŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹å¸‚åœºæ•°æ®å¹¶ç»™å‡ºäº¤æ˜“å»ºè®®ã€‚

å½“å‰ä»·æ ¼: $65,000
24hæ¶¨è·Œ: +2.5%
24hæˆäº¤é‡: $1.2B
RSI: 65.3
MACD: æ­£ä¿¡å·

è¯·ç»™å‡ºäº¤æ˜“å»ºè®®ï¼ˆBUY/SELL/HOLDï¼‰å¹¶è¯´æ˜ç†ç”±ã€‚"""
    
    print("\nğŸ“Š æµ‹è¯•æ¨ç†...")
    outputs = llm.generate([prompt], sampling_params)
    
    for output in outputs:
        print(f"\nå›ç­”:\n{output.outputs[0].text}")
    
    return llm

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", type=str, 
                       default="./models/deepseek/deepseek-coder-6.7b")
    parser.add_argument("--tensor-parallel-size", type=int, default=1)
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.9)
    
    args = parser.parse_args()
    
    deploy_deepseek(
        model_path=args.model_path,
        tensor_parallel_size=args.tensor_parallel_size,
        gpu_memory_utilization=args.gpu_memory_utilization
    )
```

è¿è¡Œéƒ¨ç½²ï¼š
```bash
python scripts/deploy_deepseek_local.py \
    --model-path ./models/deepseek/deepseek-coder-6.7b \
    --tensor-parallel-size 1
```

### 2.3 ä½¿ç”¨Transformerséƒ¨ç½²ï¼ˆç®€å•æ–¹å¼ï¼‰

åˆ›å»º `ai-inference-service/app/clients/deepseek_local_client.py`ï¼š

```python
"""
DeepSeekæœ¬åœ°å®¢æˆ·ç«¯
ä½¿ç”¨Transformersåº“åŠ è½½å’Œæ¨ç†
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)


class DeepSeekLocalClient:
    """DeepSeekæœ¬åœ°éƒ¨ç½²å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        model_path: str = "./models/deepseek/deepseek-coder-6.7b",
        device: str = "cuda",
        load_in_8bit: bool = False,
        load_in_4bit: bool = False
    ):
        """
        åˆå§‹åŒ–æœ¬åœ°DeepSeekå®¢æˆ·ç«¯
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
            load_in_8bit: æ˜¯å¦ä½¿ç”¨8ä½é‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
            load_in_4bit: æ˜¯å¦ä½¿ç”¨4ä½é‡åŒ–ï¼ˆæ›´èŠ‚çœæ˜¾å­˜ï¼‰
        """
        self.model_path = model_path
        self.device = device if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ğŸš€ åŠ è½½DeepSeekæ¨¡å‹: {model_path}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹
        from transformers import BitsAndBytesConfig
        
        if load_in_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16
            )
        elif load_in_8bit:
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        else:
            quantization_config = None
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            quantization_config=quantization_config
        )
        
        if self.device == "cpu":
            self.model = self.model.to(self.device)
        
        self.model.eval()
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def analyze_trading_signal(
        self,
        market_data: Dict[str, Any],
        features: Dict[str, float],
        sentiment_data: Optional[Dict[str, Any]] = None,
        on_chain_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        åˆ†æäº¤æ˜“ä¿¡å·
        
        Args:
            market_data: å¸‚åœºæ•°æ®
            features: æŠ€æœ¯æŒ‡æ ‡
            sentiment_data: æƒ…ç»ªæ•°æ®
            on_chain_data: é“¾ä¸Šæ•°æ®
            
        Returns:
            äº¤æ˜“ä¿¡å·å­—å…¸
        """
        # æ„å»ºæç¤ºè¯
        prompt = self._build_analysis_prompt(
            market_data, features, sentiment_data, on_chain_data
        )
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.device)
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.95,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç å“åº”
        response_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        # è§£æå“åº”
        result = self._parse_response(response_text)
        
        return result
    
    def _build_analysis_prompt(
        self,
        market_data: Dict[str, Any],
        features: Dict[str, float],
        sentiment_data: Optional[Dict[str, Any]],
        on_chain_data: Optional[Dict[str, Any]]
    ) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“AIåŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹æ•°æ®ï¼Œç»™å‡ºäº¤æ˜“å»ºè®®ã€‚

## å¸‚åœºæ•°æ®
- äº¤æ˜“å¯¹: {market_data.get('symbol', 'UNKNOWN')}
- å½“å‰ä»·æ ¼: ${market_data.get('current_price', 0):.2f}
- 24hæ¶¨è·Œ: {market_data.get('change_24h', 0):.2f}%
- 24hæˆäº¤é‡: ${market_data.get('volume_24h', 0):,.0f}
- 24hæœ€é«˜: ${market_data.get('high_24h', 0):.2f}
- 24hæœ€ä½: ${market_data.get('low_24h', 0):.2f}

## æŠ€æœ¯æŒ‡æ ‡
"""
        
        for key, value in features.items():
            if isinstance(value, (int, float)):
                prompt += f"- {key}: {value:.2f}\n"
        
        if sentiment_data:
            prompt += f"\n## å¸‚åœºæƒ…ç»ª\n"
            prompt += f"- ææƒ§è´ªå©ªæŒ‡æ•°: {sentiment_data.get('fear_greed_index', 50)}\n"
        
        if on_chain_data:
            prompt += f"\n## é“¾ä¸Šæ•°æ®\n"
            prompt += f"- äº¤æ˜“æ‰€æµå…¥: ${on_chain_data.get('exchange_inflow', 0):,.0f}\n"
        
        prompt += """
## ä»»åŠ¡è¦æ±‚
è¯·ç»¼åˆåˆ†æä»¥ä¸Šæ‰€æœ‰æ•°æ®ï¼Œç»™å‡ºäº¤æ˜“å»ºè®®ã€‚æ³¨æ„ï¼š
1. è€ƒè™‘å¤šä¸ªæ—¶é—´å‘¨æœŸ
2. è¯†åˆ«å…³é”®æ”¯æ’‘ä½å’Œé˜»åŠ›ä½
3. è¯„ä¼°å¸‚åœºæƒ…ç»ªå’Œèµ„é‡‘æµå‘
4. ç»™å‡ºæ¸…æ™°çš„é£é™©æ§åˆ¶å»ºè®®

## è¾“å‡ºæ ¼å¼
è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
    "signal": "BUY" æˆ– "SELL" æˆ– "HOLD",
    "confidence": 0.0åˆ°1.0çš„æ•°å­—,
    "position_size": 0.0åˆ°1.0çš„æ•°å­—,
    "stop_loss": æ­¢æŸä»·æ ¼,
    "take_profit": æ­¢ç›ˆä»·æ ¼,
    "reasoning": "è¯¦ç»†çš„åˆ†æç†ç”±"
}

è¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚
"""
        return prompt
    
    def _parse_response(self, response_text: str) -> Dict[str, Any]:
        """è§£æå“åº”"""
        import json
        import re
        
        # å°è¯•æå–JSON
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            try:
                result = json.loads(json_match.group())
                return result
            except:
                pass
        
        # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        return {
            "signal": "HOLD",
            "confidence": 0.5,
            "position_size": 0.0,
            "stop_loss": None,
            "take_profit": None,
            "reasoning": response_text
        }
```

---

## ğŸ“ ç¬¬ä¸‰æ­¥ï¼šå¾®è°ƒï¼ˆFine-tuningï¼‰è®­ç»ƒ

### 3.1 å‡†å¤‡è®­ç»ƒæ•°æ®

åˆ›å»ºå¾®è°ƒæ•°æ®é›† `ai-inference-service/scripts/prepare_deepseek_training_data.py`ï¼š

```python
"""
å‡†å¤‡DeepSeekå¾®è°ƒè®­ç»ƒæ•°æ®
å°†äº¤æ˜“å†å²æ•°æ®è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
"""
import json
import pandas as pd
from typing import List, Dict
import argparse


def create_training_examples(
    historical_data_path: str,
    output_path: str,
    num_examples: int = 10000
):
    """
    åˆ›å»ºå¾®è°ƒè®­ç»ƒæ ·æœ¬
    
    æ ¼å¼ï¼š
    {
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“AIåŠ©æ‰‹..."
            },
            {
                "role": "user",
                "content": "å¸‚åœºæ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡..."
            },
            {
                "role": "assistant",
                "content": "{\"signal\": \"BUY\", ...}"
            }
        ]
    }
    """
    # åŠ è½½å†å²æ•°æ®
    df = pd.read_csv(historical_data_path)
    
    examples = []
    
    for i in range(min(num_examples, len(df) - 100)):
        # è·å–å½“å‰æ•°æ®ç‚¹
        current = df.iloc[i]
        
        # æ„å»ºç”¨æˆ·è¾“å…¥ï¼ˆå¸‚åœºæ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡ï¼‰
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹å¸‚åœºæ•°æ®å¹¶ç»™å‡ºäº¤æ˜“å»ºè®®ã€‚

å½“å‰ä»·æ ¼: ${current['close']:.2f}
24hæ¶¨è·Œ: {((current['close'] / df.iloc[max(0, i-288)]['close'] - 1) * 100):.2f}%
24hæˆäº¤é‡: ${current.get('volume', 0):,.0f}
RSI: {current.get('rsi', 50):.2f}
MACD: {current.get('macd', 0):.2f}
"""
        
        # æ„å»ºåŠ©æ‰‹å›å¤ï¼ˆåŸºäºæœªæ¥ä»·æ ¼å˜åŠ¨ï¼‰
        future_price = df.iloc[min(i + 12, len(df) - 1)]['close']
        price_change = (future_price / current['close'] - 1) * 100
        
        if price_change > 1.0:
            signal = "BUY"
            confidence = min(0.9, 0.5 + abs(price_change) / 10)
        elif price_change < -1.0:
            signal = "SELL"
            confidence = min(0.9, 0.5 + abs(price_change) / 10)
        else:
            signal = "HOLD"
            confidence = 0.5
        
        assistant_response = json.dumps({
            "signal": signal,
            "confidence": confidence,
            "position_size": min(0.3, abs(price_change) / 5),
            "stop_loss": current['close'] * 0.98 if signal == "BUY" else None,
            "take_profit": current['close'] * 1.02 if signal == "BUY" else None,
            "reasoning": f"åŸºäºæŠ€æœ¯åˆ†æï¼Œé¢„æµ‹ä»·æ ¼å°†{'ä¸Šæ¶¨' if price_change > 0 else 'ä¸‹è·Œ'} {abs(price_change):.2f}%"
        })
        
        examples.append({
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“AIåŠ©æ‰‹ï¼Œæ“…é•¿æŠ€æœ¯åˆ†æå’Œé£é™©æ§åˆ¶ã€‚"
                },
                {
                    "role": "user",
                    "content": user_prompt
                },
                {
                    "role": "assistant",
                    "content": assistant_response
                }
            ]
        })
    
    # ä¿å­˜ä¸ºJSONLæ ¼å¼
    with open(output_path, 'w', encoding='utf-8') as f:
        for example in examples:
            f.write(json.dumps(example, ensure_ascii=False) + '\n')
    
    print(f"âœ… å·²åˆ›å»º {len(examples)} ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä¿å­˜åˆ°: {output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--historical-data", type=str, required=True,
                       help="å†å²æ•°æ®CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", type=str, required=True,
                       help="è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--num-examples", type=int, default=10000,
                       help="è®­ç»ƒæ ·æœ¬æ•°é‡")
    
    args = parser.parse_args()
    
    create_training_examples(
        args.historical_data,
        args.output,
        args.num_examples
    )
```

### 3.2 ä½¿ç”¨LoRAå¾®è°ƒï¼ˆæ¨èï¼‰

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œåªéœ€è¦è®­ç»ƒå°‘é‡å‚æ•°ã€‚

åˆ›å»º `ai-inference-service/scripts/finetune_deepseek_lora.py`ï¼š

```python
"""
DeepSeek LoRAå¾®è°ƒè„šæœ¬
ä½¿ç”¨PEFTåº“è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
"""
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
import argparse


def setup_model_and_tokenizer(model_path: str):
    """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
    print(f"ğŸš€ åŠ è½½æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    return model, tokenizer


def setup_lora(model):
    """è®¾ç½®LoRAé…ç½®"""
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,  # LoRA rank
        lora_alpha=32,  # LoRA alpha
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]  # æ³¨æ„åŠ›å±‚
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    return model


def prepare_dataset(jsonl_path: str, tokenizer):
    """å‡†å¤‡æ•°æ®é›†"""
    dataset = load_dataset("json", data_files=jsonl_path, split="train")
    
    def tokenize_function(examples):
        # å°†å¯¹è¯æ ¼å¼è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        text = tokenizer.apply_chat_template(
            examples["messages"],
            tokenize=False,
            add_generation_prompt=False
        )
        return tokenizer(text, truncation=True, max_length=2048)
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=False,
        remove_columns=dataset.column_names
    )
    
    return tokenized_dataset


def train(
    model_path: str,
    train_data_path: str,
    output_dir: str,
    num_epochs: int = 3,
    batch_size: int = 4,
    learning_rate: float = 2e-4
):
    """è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®æ¨¡å‹å’Œtokenizer
    model, tokenizer = setup_model_and_tokenizer(model_path)
    
    # è®¾ç½®LoRA
    model = setup_lora(model)
    
    # å‡†å¤‡æ•°æ®é›†
    dataset = prepare_dataset(train_data_path, tokenizer)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=4,
        learning_rate=learning_rate,
        fp16=True,
        logging_steps=10,
        save_steps=100,
        save_total_limit=3,
        warmup_steps=100,
        report_to="tensorboard"
    )
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model()
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", type=str, required=True,
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--train-data", type=str, required=True,
                       help="è®­ç»ƒæ•°æ®JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output-dir", type=str, required=True,
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--epochs", type=int, default=3,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=4,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning-rate", type=float, default=2e-4,
                       help="å­¦ä¹ ç‡")
    
    args = parser.parse_args()
    
    train(
        model_path=args.model_path,
        train_data_path=args.train_data,
        output_dir=args.output_dir,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate
    )
```

### 3.3 æ‰§è¡Œå¾®è°ƒè®­ç»ƒ

```bash
# 1. å‡†å¤‡è®­ç»ƒæ•°æ®
python scripts/prepare_deepseek_training_data.py \
    --historical-data data/processed/BTC_training_data.pkl \
    --output data/deepseek_training.jsonl \
    --num-examples 10000

# 2. æ‰§è¡ŒLoRAå¾®è°ƒ
python scripts/finetune_deepseek_lora.py \
    --model-path ./models/deepseek/deepseek-coder-6.7b \
    --train-data data/deepseek_training.jsonl \
    --output-dir ./models/deepseek/deepseek-coder-6.7b-finetuned \
    --epochs 3 \
    --batch-size 4 \
    --learning-rate 2e-4
```

**è®­ç»ƒæ—¶é—´ä¼°ç®—**ï¼š
- DeepSeek-Coder-6.7B + LoRA: 6-12å°æ—¶ï¼ˆå•å¡A100ï¼‰
- DeepSeek-Coder-1.3B + LoRA: 2-4å°æ—¶ï¼ˆå•å¡RTX 4090ï¼‰

---

## ğŸ”§ ç¬¬å››æ­¥ï¼šé›†æˆåˆ°AIæ¨ç†æœåŠ¡

### 4.1 ä¿®æ”¹DeepSeekå®¢æˆ·ç«¯æ”¯æŒæœ¬åœ°æ¨¡å‹

æ›´æ–° `ai-inference-service/app/clients/deepseek_client.py`ï¼Œæ·»åŠ æœ¬åœ°æ¨¡å‹æ”¯æŒï¼š

```python
# åœ¨DeepSeekClientç±»ä¸­æ·»åŠ 
def __init__(
    self,
    api_key: Optional[str] = None,  # å¯é€‰ï¼Œå¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å‹
    local_model_path: Optional[str] = None,  # æœ¬åœ°æ¨¡å‹è·¯å¾„
    use_local: bool = False,  # æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹
    ...
):
    if use_local and local_model_path:
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        from .deepseek_local_client import DeepSeekLocalClient
        self.local_client = DeepSeekLocalClient(local_model_path)
        self.use_local = True
    else:
        # ä½¿ç”¨API
        self.client = AsyncOpenAI(...)
        self.use_local = False
```

### 4.2 æ›´æ–°ç¯å¢ƒé…ç½®

åœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ ï¼š

```bash
# DeepSeeké…ç½®
DEEPSEEK_USE_LOCAL=true
DEEPSEEK_LOCAL_MODEL_PATH=./models/deepseek/deepseek-coder-6.7b-finetuned
DEEPSEEK_API_KEY=your_api_key_here  # å¤‡ç”¨
```

### 4.3 æ›´æ–°æ··åˆæ¨ç†æœåŠ¡

ä¿®æ”¹ `hybrid_inference_service.py`ï¼Œæ”¯æŒæœ¬åœ°DeepSeekï¼š

```python
# åœ¨åˆå§‹åŒ–æ—¶æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹
if os.getenv("DEEPSEEK_USE_LOCAL", "false").lower() == "true":
    from ..clients.deepseek_local_client import DeepSeekLocalClient
    self.deepseek = DeepSeekLocalClient(
        model_path=os.getenv("DEEPSEEK_LOCAL_MODEL_PATH")
    )
else:
    self.deepseek = DeepSeekClient(api_key=deepseek_api_key)
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | å»¶è¿Ÿ | æˆæœ¬ | å‡†ç¡®åº¦ | éšç§ |
|------|------|------|--------|------|
| **DeepSeek API** | 100-500ms | æŒ‰è°ƒç”¨ä»˜è´¹ | é«˜ | æ•°æ®ä¸Šä¼ äº‘ç«¯ |
| **æœ¬åœ°DeepSeekï¼ˆåŸå§‹ï¼‰** | 50-200ms | ç¡¬ä»¶æˆæœ¬ | é«˜ | å®Œå…¨æœ¬åœ° |
| **æœ¬åœ°DeepSeekï¼ˆå¾®è°ƒï¼‰** | 50-200ms | ç¡¬ä»¶æˆæœ¬ | æ›´é«˜ | å®Œå…¨æœ¬åœ° |

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### å¼€å‘/æµ‹è¯•ç¯å¢ƒ
- ä½¿ç”¨DeepSeek APIï¼ˆå¿«é€Ÿå¼€å§‹ï¼‰

### ç”Ÿäº§ç¯å¢ƒï¼ˆæ•°æ®æ•æ„Ÿï¼‰
- ä½¿ç”¨æœ¬åœ°DeepSeek + LoRAå¾®è°ƒ
- ä¼˜åŠ¿ï¼šæ•°æ®éšç§ + é’ˆå¯¹äº¤æ˜“åœºæ™¯ä¼˜åŒ–

### æ··åˆæ–¹æ¡ˆ
- ç®€å•åœºæ™¯ï¼šæœ¬åœ°æ¨¡å‹
- å¤æ‚åœºæ™¯ï¼šæœ¬åœ°DeepSeekï¼ˆå¾®è°ƒåï¼‰
- é™çº§ï¼šDeepSeek API

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. ä¸‹è½½æ¨¡å‹
cd ai-inference-service
huggingface-cli download deepseek-ai/deepseek-coder-6.7b-instruct \
    --local-dir ./models/deepseek/deepseek-coder-6.7b

# 2. å‡†å¤‡è®­ç»ƒæ•°æ®
python scripts/prepare_deepseek_training_data.py \
    --historical-data data/historical/BTC-USDT_5m_2024.csv \
    --output data/deepseek_training.jsonl

# 3. å¾®è°ƒè®­ç»ƒï¼ˆå¯é€‰ï¼‰
python scripts/finetune_deepseek_lora.py \
    --model-path ./models/deepseek/deepseek-coder-6.7b \
    --train-data data/deepseek_training.jsonl \
    --output-dir ./models/deepseek/deepseek-coder-6.7b-finetuned

# 4. é…ç½®ç¯å¢ƒå˜é‡
echo "DEEPSEEK_USE_LOCAL=true" >> .env
echo "DEEPSEEK_LOCAL_MODEL_PATH=./models/deepseek/deepseek-coder-6.7b-finetuned" >> .env

# 5. å¯åŠ¨æœåŠ¡
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [DeepSeek GitHub](https://github.com/deepseek-ai)
- [DeepSeekæ¨¡å‹HuggingFace](https://huggingface.co/deepseek-ai)
- [vLLMæ–‡æ¡£](https://docs.vllm.ai/)
- [PEFTæ–‡æ¡£](https://huggingface.co/docs/peft/)

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€**

