#!/usr/bin/env python3
"""
å‡†å¤‡è®­ç»ƒæ•°æ®è„šæœ¬
åŒ…å«æ•°æ®æ¸…æ´—ã€ç‰¹å¾è®¡ç®—ã€æ ‡ç­¾ç”Ÿæˆå’Œæ•°æ®é›†åˆ’åˆ†
"""

import argparse
import pandas as pd
import numpy as np
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent))

from app.features.feature_engineer import FeatureEngineer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pickle


def load_historical_data(file_path: str) -> pd.DataFrame:
    """
    åŠ è½½å†å²æ•°æ®
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        DataFrame
    """
    print(f"ğŸ“¥ åŠ è½½æ•°æ®: {file_path}")
    
    if file_path.endswith('.csv'):
        df = pd.read_csv(file_path)
    elif file_path.endswith('.parquet'):
        df = pd.read_parquet(file_path)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
    
    # ç¡®ä¿timestampåˆ—æ˜¯datetimeç±»å‹
    if 'timestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # æŒ‰æ—¶é—´æ’åº
    df = df.sort_values('timestamp')
    
    print(f"âœ… æ•°æ®å·²åŠ è½½: {len(df)} æ¡è®°å½•")
    print(f"   æ—¶é—´èŒƒå›´: {df['timestamp'].min()} â†’ {df['timestamp'].max()}")
    
    return df


def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ¸…æ´—æ•°æ®
    """
    print("\nğŸ§¹ æ¸…æ´—æ•°æ®...")
    
    original_count = len(df)
    
    # 1. å»é™¤é‡å¤
    df = df.drop_duplicates(subset=['timestamp'])
    
    # 2. å»é™¤ç¼ºå¤±å€¼
    df = df.dropna()
    
    # 3. éªŒè¯OHLCå…³ç³»
    df = df[
        (df['low'] <= df['open']) &
        (df['low'] <= df['close']) &
        (df['high'] >= df['open']) &
        (df['high'] >= df['close']) &
        (df['volume'] >= 0)
    ]
    
    # 4. å»é™¤å¼‚å¸¸å€¼ï¼ˆåŸºäº3sigmaè§„åˆ™ï¼‰
    for col in ['open', 'high', 'low', 'close']:
        mean = df[col].mean()
        std = df[col].std()
        df = df[(df[col] >= mean - 3*std) & (df[col] <= mean + 3*std)]
    
    print(f"âœ… æ¸…æ´—å®Œæˆ: {original_count} â†’ {len(df)} æ¡è®°å½•")
    
    return df


def calculate_features(df: pd.DataFrame) -> tuple:
    """
    è®¡ç®—ç‰¹å¾
    
    Returns:
        (features_list, timestamps)
    """
    print("\nğŸ”§ è®¡ç®—ç‰¹å¾...")
    
    engineer = FeatureEngineer()
    features_list = []
    timestamps = []
    
    # éœ€è¦è‡³å°‘24å°æ—¶çš„æ•°æ®ï¼ˆ288ä¸ª5åˆ†é’ŸKçº¿ï¼‰
    window_size = 288
    
    for i in range(window_size, len(df)):
        if i % 1000 == 0:
            print(f"   è¿›åº¦: {i}/{len(df)}")
        
        try:
            prices_24h = df['close'].iloc[i-window_size:i].tolist()
            prices_1h = df['close'].iloc[i-12:i].tolist()  # 12ä¸ª5åˆ†é’ŸKçº¿ = 1å°æ—¶
            volumes_24h = df['volume'].iloc[i-window_size:i].tolist()
            current_price = df['close'].iloc[i]
            
            # è®¡ç®—ç‰¹å¾
            features = engineer.extract_features(
                current_price=current_price,
                prices_1h=prices_1h,
                prices_24h=prices_24h,
                volumes_24h=volumes_24h,
                bid_ask_spread=0.01,  # å‡è®¾å€¼
                funding_rate=0.0
            )
            
            # è½¬æ¢ä¸ºæ•°ç»„
            feature_array = engineer.to_array(features)
            features_list.append(feature_array)
            timestamps.append(df['timestamp'].iloc[i])
            
        except Exception as e:
            print(f"   âš ï¸  ç¬¬{i}è¡Œç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
            continue
    
    print(f"âœ… ç‰¹å¾è®¡ç®—å®Œæˆ: {len(features_list)} ä¸ªæ ·æœ¬")
    
    return np.array(features_list), timestamps


def generate_labels(
    df: pd.DataFrame,
    start_idx: int = 288,
    forward_window: int = 12,
    threshold: float = 1.0
) -> np.ndarray:
    """
    ç”Ÿæˆè®­ç»ƒæ ‡ç­¾
    
    Args:
        df: åŸå§‹æ•°æ®
        start_idx: å¼€å§‹ç´¢å¼•
        forward_window: å‰ç»çª—å£ï¼ˆé»˜è®¤12ä¸ª5åˆ†é’Ÿ=1å°æ—¶ï¼‰
        threshold: æ¶¨è·Œé˜ˆå€¼ï¼ˆé»˜è®¤1%ï¼‰
        
    Returns:
        æ ‡ç­¾æ•°ç»„ (0: BUY, 1: HOLD, 2: SELL)
    """
    print(f"\nğŸ·ï¸  ç”Ÿæˆè®­ç»ƒæ ‡ç­¾...")
    print(f"   å‰ç»çª—å£: {forward_window} (çº¦{forward_window*5}åˆ†é’Ÿ)")
    print(f"   æ¶¨è·Œé˜ˆå€¼: {threshold}%")
    
    labels = []
    
    for i in range(start_idx, len(df) - forward_window):
        if i % 1000 == 0:
            print(f"   è¿›åº¦: {i - start_idx}/{len(df) - start_idx - forward_window}")
        
        current_price = df['close'].iloc[i]
        future_price = df['close'].iloc[i + forward_window]
        
        change_pct = (future_price - current_price) / current_price * 100
        
        if change_pct > threshold:
            labels.append(0)  # BUY
        elif change_pct < -threshold:
            labels.append(2)  # SELL
        else:
            labels.append(1)  # HOLD
    
    labels = np.array(labels)
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    buy_count = np.sum(labels == 0)
    hold_count = np.sum(labels == 1)
    sell_count = np.sum(labels == 2)
    total = len(labels)
    
    print(f"âœ… æ ‡ç­¾ç”Ÿæˆå®Œæˆ: {total} ä¸ªæ ·æœ¬")
    print(f"   BUY:  {buy_count} ({buy_count/total*100:.1f}%)")
    print(f"   HOLD: {hold_count} ({hold_count/total*100:.1f}%)")
    print(f"   SELL: {sell_count} ({sell_count/total*100:.1f}%)")
    
    return labels


def split_dataset(
    features: np.ndarray,
    labels: np.ndarray,
    timestamps: list,
    test_size: float = 0.2,
    val_size: float = 0.1
) -> dict:
    """
    åˆ’åˆ†æ•°æ®é›†
    
    Args:
        features: ç‰¹å¾çŸ©é˜µ
        labels: æ ‡ç­¾å‘é‡
        timestamps: æ—¶é—´æˆ³åˆ—è¡¨
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        val_size: éªŒè¯é›†æ¯”ä¾‹
        
    Returns:
        åŒ…å«train/val/testæ•°æ®çš„å­—å…¸
    """
    print(f"\nğŸ“Š åˆ’åˆ†æ•°æ®é›†...")
    print(f"   æµ‹è¯•é›†: {test_size*100}%")
    print(f"   éªŒè¯é›†: {val_size*100}%")
    print(f"   è®­ç»ƒé›†: {(1-test_size-val_size)*100}%")
    
    # å…ˆåˆ’åˆ†å‡ºæµ‹è¯•é›†
    X_temp, X_test, y_temp, y_test, ts_temp, ts_test = train_test_split(
        features, labels, timestamps,
        test_size=test_size,
        shuffle=False  # æ—¶åºæ•°æ®ä¸æ‰“ä¹±
    )
    
    # å†ä»å‰©ä½™æ•°æ®ä¸­åˆ’åˆ†éªŒè¯é›†
    val_ratio = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val, ts_train, ts_val = train_test_split(
        X_temp, y_temp, ts_temp,
        test_size=val_ratio,
        shuffle=False
    )
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    return {
        'X_train': X_train_scaled,
        'y_train': y_train,
        'ts_train': ts_train,
        'X_val': X_val_scaled,
        'y_val': y_val,
        'ts_val': ts_val,
        'X_test': X_test_scaled,
        'y_test': y_test,
        'ts_test': ts_test,
        'scaler': scaler,
        'feature_names': FeatureEngineer().get_feature_names()
    }


def save_processed_data(dataset: dict, output_path: str):
    """
    ä¿å­˜å¤„ç†å¥½çš„æ•°æ®
    """
    print(f"\nğŸ’¾ ä¿å­˜å¤„ç†æ•°æ®: {output_path}")
    
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'wb') as f:
        pickle.dump(dataset, f)
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜")
    
    # ä¿å­˜feature_nameså•ç‹¬æ–‡ä»¶ï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
    feature_names_file = output_file.parent / 'feature_names.txt'
    with open(feature_names_file, 'w') as f:
        for i, name in enumerate(dataset['feature_names']):
            f.write(f"{i+1}. {name}\n")
    
    print(f"   ç‰¹å¾åç§°å·²ä¿å­˜åˆ°: {feature_names_file}")


def main():
    parser = argparse.ArgumentParser(description='å‡†å¤‡AIæ¨¡å‹è®­ç»ƒæ•°æ®')
    
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='è¾“å…¥æ•°æ®æ–‡ä»¶ï¼ˆCSVæˆ–Parquetï¼‰'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='data/processed/training_data.pkl',
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šdata/processed/training_data.pklï¼‰'
    )
    
    parser.add_argument(
        '--threshold',
        type=float,
        default=1.0,
        help='æ ‡ç­¾ç”Ÿæˆé˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼Œé»˜è®¤1.0ï¼‰'
    )
    
    parser.add_argument(
        '--forward-window',
        type=int,
        default=12,
        help='å‰ç»çª—å£å¤§å°ï¼ˆ5åˆ†é’ŸKçº¿æ•°é‡ï¼Œé»˜è®¤12=1å°æ—¶ï¼‰'
    )
    
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.2ï¼‰'
    )
    
    parser.add_argument(
        '--val-size',
        type=float,
        default=0.1,
        help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.1ï¼‰'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ AIäº¤æ˜“ç³»ç»Ÿ - æ•°æ®å‡†å¤‡Pipeline")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    df = load_historical_data(args.input)
    
    # 2. æ¸…æ´—æ•°æ®
    df = clean_data(df)
    
    # 3. è®¡ç®—ç‰¹å¾
    features, timestamps = calculate_features(df)
    
    # 4. ç”Ÿæˆæ ‡ç­¾
    labels = generate_labels(
        df,
        start_idx=288,
        forward_window=args.forward_window,
        threshold=args.threshold
    )
    
    # ç¡®ä¿ç‰¹å¾å’Œæ ‡ç­¾æ•°é‡ä¸€è‡´
    min_len = min(len(features), len(labels), len(timestamps))
    features = features[:min_len]
    labels = labels[:min_len]
    timestamps = timestamps[:min_len]
    
    # 5. åˆ’åˆ†æ•°æ®é›†
    dataset = split_dataset(
        features,
        labels,
        timestamps,
        test_size=args.test_size,
        val_size=args.val_size
    )
    
    # 6. ä¿å­˜æ•°æ®
    save_processed_data(dataset, args.output)
    
    print("\n" + "=" * 60)
    print("âœ¨ æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print("=" * 60)
    print(f"\nä¸‹ä¸€æ­¥: ä½¿ç”¨ python scripts/train_models.py --data {args.output} è®­ç»ƒæ¨¡å‹")


if __name__ == "__main__":
    main()

