#!/usr/bin/env python3
"""
AIæ¨¡å‹è®­ç»ƒè„šæœ¬
è®­ç»ƒLSTMã€Transformerå’ŒRandom Forestæ¨¡å‹
"""

import argparse
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import pickle
import torch
from torch.utils.data import TensorDataset, DataLoader
import numpy as np

from app.models.lstm_model import LSTMModelManager
from app.models.transformer_model import TransformerModelManager
from app.models.random_forest_model import RandomForestModelManager


def load_processed_data(data_path: str) -> dict:
    """
    åŠ è½½å¤„ç†å¥½çš„æ•°æ®
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ•°æ®å­—å…¸
    """
    print(f"ğŸ“¥ åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    
    with open(data_path, 'rb') as f:
        dataset = pickle.load(f)
    
    print(f"âœ… æ•°æ®å·²åŠ è½½:")
    print(f"   è®­ç»ƒé›†: {len(dataset['X_train'])} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(dataset['X_val'])} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(dataset['X_test'])} æ ·æœ¬")
    print(f"   ç‰¹å¾æ•°: {dataset['X_train'].shape[1]}")
    
    return dataset


def create_sequences(X: np.ndarray, y: np.ndarray, sequence_length: int = 12) -> tuple:
    """
    åˆ›å»ºæ—¶åºåºåˆ—ï¼ˆç”¨äºLSTMå’ŒTransformerï¼‰
    
    Args:
        X: ç‰¹å¾çŸ©é˜µ
        y: æ ‡ç­¾å‘é‡
        sequence_length: åºåˆ—é•¿åº¦
        
    Returns:
        (X_sequences, y_sequences)
    """
    X_seq = []
    y_seq = []
    
    for i in range(len(X) - sequence_length):
        X_seq.append(X[i:i+sequence_length])
        y_seq.append(y[i+sequence_length])
    
    return np.array(X_seq), np.array(y_seq)


def train_lstm(dataset: dict, epochs: int = 50, batch_size: int = 64):
    """
    è®­ç»ƒLSTMæ¨¡å‹
    
    Args:
        dataset: æ•°æ®é›†
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    print("\n" + "=" * 60)
    print("ğŸ¤– è®­ç»ƒLSTMæ¨¡å‹")
    print("=" * 60)
    
    # åˆ›å»ºåºåˆ—
    print("\nåˆ›å»ºæ—¶åºåºåˆ—...")
    X_train_seq, y_train_seq = create_sequences(dataset['X_train'], dataset['y_train'], sequence_length=12)
    X_val_seq, y_val_seq = create_sequences(dataset['X_val'], dataset['y_val'], sequence_length=12)
    
    print(f"è®­ç»ƒåºåˆ—: {X_train_seq.shape}")
    print(f"éªŒè¯åºåˆ—: {X_val_seq.shape}")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train_seq),
        torch.LongTensor(y_train_seq)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(X_val_seq),
        torch.LongTensor(y_val_seq)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    manager = LSTMModelManager(model_path="models/lstm_model.pth")
    manager.scaler = dataset['scaler']  # è®¾ç½®scaler
    
    # è®­ç»ƒ
    manager.train_model(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=epochs,
        learning_rate=0.001
    )
    
    print("\nâœ… LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")


def train_transformer(dataset: dict, epochs: int = 50, batch_size: int = 32):
    """
    è®­ç»ƒTransformeræ¨¡å‹
    
    Args:
        dataset: æ•°æ®é›†
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    print("\n" + "=" * 60)
    print("ğŸ¤– è®­ç»ƒTransformeræ¨¡å‹")
    print("=" * 60)
    
    # åˆ›å»ºåºåˆ—ï¼ˆTransformerä½¿ç”¨æ›´é•¿çš„åºåˆ—ï¼‰
    print("\nåˆ›å»ºæ—¶åºåºåˆ—...")
    X_train_seq, y_train_seq = create_sequences(dataset['X_train'], dataset['y_train'], sequence_length=24)
    X_val_seq, y_val_seq = create_sequences(dataset['X_val'], dataset['y_val'], sequence_length=24)
    
    print(f"è®­ç»ƒåºåˆ—: {X_train_seq.shape}")
    print(f"éªŒè¯åºåˆ—: {X_val_seq.shape}")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train_seq),
        torch.LongTensor(y_train_seq)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(X_val_seq),
        torch.LongTensor(y_val_seq)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    manager = TransformerModelManager(model_path="models/transformer_model.pth")
    manager.scaler = dataset['scaler']
    
    # è®­ç»ƒ
    manager.train_model(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=epochs,
        learning_rate=0.0001
    )
    
    print("\nâœ… Transformeræ¨¡å‹è®­ç»ƒå®Œæˆ")


def train_random_forest(dataset: dict):
    """
    è®­ç»ƒRandom Forestæ¨¡å‹
    
    Args:
        dataset: æ•°æ®é›†
    """
    print("\n" + "=" * 60)
    print("ğŸ¤– è®­ç»ƒRandom Forestæ¨¡å‹")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    manager = RandomForestModelManager(model_path="models/random_forest_model.pkl")
    
    # è®­ç»ƒ
    metrics = manager.train_model(
        X_train=dataset['X_train'],
        y_train=dataset['y_train'],
        X_val=dataset['X_val'],
        y_val=dataset['y_val'],
        feature_names=dataset['feature_names']
    )
    
    print("\nâœ… Random Forestæ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    return metrics


def evaluate_on_test_set(dataset: dict):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ‰€æœ‰æ¨¡å‹
    
    Args:
        dataset: æ•°æ®é›†
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•é›†è¯„ä¼°")
    print("=" * 60)
    
    from sklearn.metrics import accuracy_score, classification_report
    
    X_test = dataset['X_test']
    y_test = dataset['y_test']
    
    # 1. Random Forestè¯„ä¼°
    print("\n1ï¸âƒ£  Random Forest:")
    rf_manager = RandomForestModelManager(model_path="models/random_forest_model.pkl")
    
    try:
        rf_preds = []
        for x in X_test:
            signal, confidence, _ = rf_manager.predict(x)
            signal_map = {"BUY": 0, "HOLD": 1, "SELL": 2}
            rf_preds.append(signal_map[signal])
        
        rf_accuracy = accuracy_score(y_test, rf_preds)
        print(f"   å‡†ç¡®ç‡: {rf_accuracy:.4f}")
        print("\n   åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, rf_preds, target_names=['BUY', 'HOLD', 'SELL']))
    except Exception as e:
        print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
    
    # 2. LSTMè¯„ä¼°ï¼ˆéœ€è¦åˆ›å»ºåºåˆ—ï¼‰
    print("\n2ï¸âƒ£  LSTM:")
    lstm_manager = LSTMModelManager(model_path="models/lstm_model.pth")
    
    try:
        # ç”±äºLSTMéœ€è¦åºåˆ—ï¼Œè¿™é‡Œç®€åŒ–è¯„ä¼°
        print("   éœ€è¦æ—¶åºåºåˆ—ï¼Œè·³è¿‡å•ç‹¬è¯„ä¼°")
    except Exception as e:
        print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
    
    # 3. Transformerè¯„ä¼°
    print("\n3ï¸âƒ£  Transformer:")
    transformer_manager = TransformerModelManager(model_path="models/transformer_model.pth")
    
    try:
        print("   éœ€è¦æ—¶åºåºåˆ—ï¼Œè·³è¿‡å•ç‹¬è¯„ä¼°")
    except Exception as e:
        print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒAIäº¤æ˜“æ¨¡å‹')
    
    parser.add_argument(
        '--data',
        type=str,
        required=True,
        help='è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆ.pklï¼‰'
    )
    
    parser.add_argument(
        '--models',
        type=str,
        nargs='+',
        default=['lstm', 'transformer', 'rf'],
        choices=['lstm', 'transformer', 'rf', 'all'],
        help='è¦è®­ç»ƒçš„æ¨¡å‹ï¼ˆé»˜è®¤ï¼šå…¨éƒ¨ï¼‰'
    )
    
    parser.add_argument(
        '--epochs',
        type=int,
        default=50,
        help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤50ï¼‰'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=64,
        help='æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤64ï¼‰'
    )
    
    parser.add_argument(
        '--skip-evaluation',
        action='store_true',
        help='è·³è¿‡æµ‹è¯•é›†è¯„ä¼°'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ AIäº¤æ˜“ç³»ç»Ÿ - æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    dataset = load_processed_data(args.data)
    
    # ç¡®å®šè¦è®­ç»ƒçš„æ¨¡å‹
    if 'all' in args.models:
        models_to_train = ['lstm', 'transformer', 'rf']
    else:
        models_to_train = args.models
    
    print(f"\nå°†è®­ç»ƒä»¥ä¸‹æ¨¡å‹: {', '.join(models_to_train)}")
    
    # è®­ç»ƒæ¨¡å‹
    if 'lstm' in models_to_train:
        train_lstm(dataset, epochs=args.epochs, batch_size=args.batch_size)
    
    if 'transformer' in models_to_train:
        train_transformer(dataset, epochs=args.epochs, batch_size=args.batch_size // 2)
    
    if 'rf' in models_to_train:
        train_random_forest(dataset)
    
    # æµ‹è¯•é›†è¯„ä¼°
    if not args.skip_evaluation:
        evaluate_on_test_set(dataset)
    
    print("\n" + "=" * 60)
    print("âœ¨ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print("\nä¸‹ä¸€æ­¥: ä½¿ç”¨ uvicorn app.main:app å¯åŠ¨AIæ¨ç†æœåŠ¡")


if __name__ == "__main__":
    main()

