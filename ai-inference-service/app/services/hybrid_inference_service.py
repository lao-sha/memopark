"""
æ··åˆAIæ¨ç†æœåŠ¡
ç»“åˆDeepSeek APIå’Œæœ¬åœ°æ¨¡å‹ï¼Œæä¾›é«˜æ•ˆå¯é çš„äº¤æ˜“ä¿¡å·ç”Ÿæˆ

æ¶æ„ï¼š
1. DeepSeek APIï¼šå¤„ç†å¤æ‚å¸‚åœºåœºæ™¯
2. æœ¬åœ°æ¨¡å‹ï¼šå¤„ç†ç®€å•åœºæ™¯å’Œé™çº§å¤‡ä»½
3. Redisç¼“å­˜ï¼šå‡å°‘é‡å¤è®¡ç®—
4. è‡ªåŠ¨é™çº§ï¼šAPIå¤±è´¥æ—¶åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å‹
"""

import logging
import hashlib
import json
from typing import Dict, Any, Optional
from datetime import datetime, timedelta
import redis.asyncio as redis

from ..clients.deepseek_client import DeepSeekClient
from ..models.local_simple_model import LocalSimpleModel, ScenarioClassifier
from ..utils.data_anonymizer import DataAnonymizer, SensitiveDataValidator

logger = logging.getLogger(__name__)


class HybridInferenceService:
    """æ··åˆæ¨ç†æœåŠ¡"""
    
    def __init__(
        self,
        deepseek_api_key: str,
        redis_url: str = "redis://localhost:6379",
        cache_ttl: int = 60,
        enable_anonymization: bool = True,
        fallback_to_local: bool = True
    ):
        """
        åˆå§‹åŒ–æ··åˆæ¨ç†æœåŠ¡
        
        Args:
            deepseek_api_key: DeepSeek APIå¯†é’¥
            redis_url: Redisè¿æ¥URL
            cache_ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
            enable_anonymization: æ˜¯å¦å¯ç”¨æ•°æ®è„±æ•
            fallback_to_local: APIå¤±è´¥æ—¶æ˜¯å¦é™çº§åˆ°æœ¬åœ°æ¨¡å‹
        """
        # DeepSeekå®¢æˆ·ç«¯
        self.deepseek = DeepSeekClient(api_key=deepseek_api_key)
        
        # æœ¬åœ°æ¨¡å‹
        self.local_model = LocalSimpleModel()
        
        # æ•°æ®è„±æ•å™¨
        self.anonymizer = DataAnonymizer(add_noise=False) if enable_anonymization else None
        
        # Redisç¼“å­˜
        self.redis_client: Optional[redis.Redis] = None
        self.redis_url = redis_url
        self.cache_ttl = cache_ttl
        
        # é…ç½®
        self.enable_anonymization = enable_anonymization
        self.fallback_to_local = fallback_to_local
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "deepseek_calls": 0,
            "local_calls": 0,
            "fallback_calls": 0,
            "errors": 0,
        }
        
        # è¿ç»­å¤±è´¥è®¡æ•°ï¼ˆç”¨äºè‡ªåŠ¨é™çº§ï¼‰
        self.consecutive_failures = 0
        self.max_failures_before_fallback = 3
    
    async def initialize(self):
        """åˆå§‹åŒ–å¼‚æ­¥èµ„æº"""
        try:
            self.redis_client = await redis.from_url(
                self.redis_url,
                encoding="utf-8",
                decode_responses=True
            )
            logger.info("Redisè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.warning(f"Redisè¿æ¥å¤±è´¥ï¼Œç¼“å­˜åŠŸèƒ½å°†ä¸å¯ç”¨: {e}")
            self.redis_client = None
    
    async def get_trading_signal(
        self,
        market_data: Dict[str, Any],
        features: Dict[str, float],
        sentiment_data: Optional[Dict[str, Any]] = None,
        on_chain_data: Optional[Dict[str, Any]] = None,
        force_model: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è·å–äº¤æ˜“ä¿¡å·ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            market_data: å¸‚åœºæ•°æ®
            features: æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
            sentiment_data: æƒ…ç»ªæ•°æ®ï¼ˆå¯é€‰ï¼‰
            on_chain_data: é“¾ä¸Šæ•°æ®ï¼ˆå¯é€‰ï¼‰
            force_model: å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šæ¨¡å‹ ("deepseek" æˆ– "local")
            
        Returns:
            äº¤æ˜“ä¿¡å·å­—å…¸
        """
        self.stats["total_requests"] += 1
        start_time = datetime.now()
        
        try:
            # Step 1: æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(market_data, features)
            cached_result = await self._get_from_cache(cache_key)
            
            if cached_result:
                self.stats["cache_hits"] += 1
                logger.info("âœ… ç¼“å­˜å‘½ä¸­")
                return cached_result
            
            # Step 2: åœºæ™¯åˆ†ç±»ï¼ˆé™¤éå¼ºåˆ¶æŒ‡å®šæ¨¡å‹ï¼‰
            if force_model:
                complexity = force_model
                reason = f"å¼ºåˆ¶ä½¿ç”¨{force_model}æ¨¡å‹"
            else:
                complexity, reason = ScenarioClassifier.classify(market_data, features)
            
            logger.info(f"ğŸ“Š åœºæ™¯åˆ†ç±»: {complexity} - {reason}")
            
            # Step 3: æ ¹æ®åœºæ™¯é€‰æ‹©æ¨¡å‹
            if complexity == "simple" or force_model == "local":
                # ç®€å•åœºæ™¯ï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹
                result = await self._call_local_model(market_data, features)
                
            else:
                # å¤æ‚åœºæ™¯ï¼šä½¿ç”¨DeepSeek
                result = await self._call_deepseek_with_fallback(
                    market_data, features, sentiment_data, on_chain_data
                )
            
            # Step 4: ç¼“å­˜ç»“æœ
            await self._save_to_cache(cache_key, result)
            
            # æ·»åŠ å…ƒæ•°æ®
            result["metadata"] = {
                "complexity": complexity,
                "classification_reason": reason,
                "response_time_ms": (datetime.now() - start_time).total_seconds() * 1000,
                "cached": False
            }
            
            return result
            
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"æ¨ç†æœåŠ¡é”™è¯¯: {e}", exc_info=True)
            
            # æœ€ç»ˆé™çº§åˆ°æœ¬åœ°æ¨¡å‹
            if self.fallback_to_local:
                logger.warning("âš ï¸ æœ€ç»ˆé™çº§åˆ°æœ¬åœ°æ¨¡å‹")
                return await self._call_local_model(market_data, features)
            
            raise
    
    async def _call_deepseek_with_fallback(
        self,
        market_data: Dict[str, Any],
        features: Dict[str, float],
        sentiment_data: Optional[Dict[str, Any]],
        on_chain_data: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨DeepSeekï¼Œå¤±è´¥æ—¶è‡ªåŠ¨é™çº§
        
        Args:
            market_data: å¸‚åœºæ•°æ®
            features: æŠ€æœ¯æŒ‡æ ‡
            sentiment_data: æƒ…ç»ªæ•°æ®
            on_chain_data: é“¾ä¸Šæ•°æ®
            
        Returns:
            äº¤æ˜“ä¿¡å·
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨é™çº§
        if self.consecutive_failures >= self.max_failures_before_fallback:
            logger.warning(
                f"âš ï¸ DeepSeekè¿ç»­å¤±è´¥{self.consecutive_failures}æ¬¡ï¼Œ"
                f"è‡ªåŠ¨é™çº§åˆ°æœ¬åœ°æ¨¡å‹"
            )
            self.stats["fallback_calls"] += 1
            return await self._call_local_model(market_data, features)
        
        try:
            # æ•°æ®è„±æ•
            if self.enable_anonymization:
                safe_market, safe_features, safe_sentiment, safe_onchain = \
                    self.anonymizer.anonymize_request(
                        market_data, features, sentiment_data, on_chain_data
                    )
                
                # éªŒè¯æ•°æ®å®‰å…¨æ€§
                all_data = {
                    **safe_market,
                    **safe_features,
                    **(safe_sentiment or {}),
                    **(safe_onchain or {})
                }
                
                is_safe, sensitive_fields = SensitiveDataValidator.validate(all_data)
                
                if not is_safe:
                    logger.error(f"âŒ å‘ç°æ•æ„Ÿå­—æ®µ: {sensitive_fields}ï¼Œæ‹’ç»å‘é€")
                    raise ValueError(f"æ•°æ®åŒ…å«æ•æ„Ÿå­—æ®µ: {sensitive_fields}")
            else:
                safe_market = market_data
                safe_features = features
                safe_sentiment = sentiment_data
                safe_onchain = on_chain_data
            
            # è°ƒç”¨DeepSeek
            logger.info("ğŸ¤– è°ƒç”¨DeepSeek API...")
            result = await self.deepseek.analyze_trading_signal(
                market_data=safe_market,
                features=safe_features,
                sentiment_data=safe_sentiment,
                on_chain_data=safe_onchain
            )
            
            # æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°
            self.consecutive_failures = 0
            self.stats["deepseek_calls"] += 1
            
            return result
            
        except Exception as e:
            logger.error(f"DeepSeekè°ƒç”¨å¤±è´¥: {e}")
            self.consecutive_failures += 1
            
            # é™çº§åˆ°æœ¬åœ°æ¨¡å‹
            if self.fallback_to_local:
                logger.warning("âš ï¸ é™çº§åˆ°æœ¬åœ°æ¨¡å‹")
                self.stats["fallback_calls"] += 1
                return await self._call_local_model(market_data, features)
            
            raise
    
    async def _call_local_model(
        self,
        market_data: Dict[str, Any],
        features: Dict[str, float]
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨æœ¬åœ°æ¨¡å‹
        
        Args:
            market_data: å¸‚åœºæ•°æ®
            features: æŠ€æœ¯æŒ‡æ ‡
            
        Returns:
            äº¤æ˜“ä¿¡å·
        """
        logger.info("ğŸ  ä½¿ç”¨æœ¬åœ°æ¨¡å‹...")
        self.stats["local_calls"] += 1
        
        result = self.local_model.predict(market_data, features)
        return result
    
    def _generate_cache_key(
        self,
        market_data: Dict[str, Any],
        features: Dict[str, float]
    ) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        åŸºäºå¸‚åœºæ•°æ®å’Œç‰¹å¾çš„å“ˆå¸Œå€¼ï¼Œç¡®ä¿ç›¸åŒè¾“å…¥è¿”å›ç›¸åŒç»“æœ
        
        Args:
            market_data: å¸‚åœºæ•°æ®
            features: æŠ€æœ¯æŒ‡æ ‡
            
        Returns:
            ç¼“å­˜é”®å­—ç¬¦ä¸²
        """
        # åˆ›å»ºç¡®å®šæ€§çš„æ•°æ®è¡¨ç¤º
        cache_data = {
            "symbol": market_data.get("symbol"),
            "price": round(market_data.get("price", 0), 2),
            "features": {k: round(v, 2) for k, v in sorted(features.items())}
        }
        
        # ç”Ÿæˆå“ˆå¸Œ
        data_str = json.dumps(cache_data, sort_keys=True)
        hash_value = hashlib.md5(data_str.encode()).hexdigest()
        
        return f"ai_signal:{hash_value}"
    
    async def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """
        ä»ç¼“å­˜è·å–ç»“æœ
        
        Args:
            cache_key: ç¼“å­˜é”®
            
        Returns:
            ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        if not self.redis_client:
            return None
        
        try:
            cached = await self.redis_client.get(cache_key)
            if cached:
                return json.loads(cached)
        except Exception as e:
            logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        return None
    
    async def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """
        ä¿å­˜ç»“æœåˆ°ç¼“å­˜
        
        Args:
            cache_key: ç¼“å­˜é”®
            result: ç»“æœæ•°æ®
        """
        if not self.redis_client:
            return
        
        try:
            await self.redis_client.setex(
                cache_key,
                self.cache_ttl,
                json.dumps(result)
            )
        except Exception as e:
            logger.warning(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total = self.stats["total_requests"]
        
        return {
            **self.stats,
            "cache_hit_rate": (
                self.stats["cache_hits"] / total * 100 if total > 0 else 0
            ),
            "deepseek_usage_rate": (
                self.stats["deepseek_calls"] / total * 100 if total > 0 else 0
            ),
            "local_usage_rate": (
                self.stats["local_calls"] / total * 100 if total > 0 else 0
            ),
            "fallback_rate": (
                self.stats["fallback_calls"] / total * 100 if total > 0 else 0
            ),
            "error_rate": (
                self.stats["errors"] / total * 100 if total > 0 else 0
            ),
            "consecutive_failures": self.consecutive_failures,
            "deepseek_stats": self.deepseek.get_stats(),
            "local_model_stats": self.local_model.get_stats()
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """
        å¥åº·æ£€æŸ¥
        
        Returns:
            å¥åº·çŠ¶æ€å­—å…¸
        """
        health = {
            "status": "healthy",
            "components": {}
        }
        
        # æ£€æŸ¥Redis
        if self.redis_client:
            try:
                await self.redis_client.ping()
                health["components"]["redis"] = "healthy"
            except Exception as e:
                health["components"]["redis"] = f"unhealthy: {e}"
                health["status"] = "degraded"
        else:
            health["components"]["redis"] = "disabled"
        
        # æ£€æŸ¥DeepSeek
        if self.consecutive_failures >= self.max_failures_before_fallback:
            health["components"]["deepseek"] = "degraded (using fallback)"
            health["status"] = "degraded"
        else:
            health["components"]["deepseek"] = "healthy"
        
        # æœ¬åœ°æ¨¡å‹å§‹ç»ˆå¯ç”¨
        health["components"]["local_model"] = "healthy"
        
        return health
    
    async def close(self):
        """å…³é—­æœåŠ¡ï¼Œé‡Šæ”¾èµ„æº"""
        if self.redis_client:
            await self.redis_client.close()
        
        await self.deepseek.close()
        
        logger.info("æ··åˆæ¨ç†æœåŠ¡å·²å…³é—­")

