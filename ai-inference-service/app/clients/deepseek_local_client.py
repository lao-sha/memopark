"""
DeepSeekæœ¬åœ°å®¢æˆ·ç«¯
ä½¿ç”¨Transformersåº“åŠ è½½å’Œæ¨ç†æœ¬åœ°DeepSeekæ¨¡å‹
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Dict, Any, Optional
import logging
import json
import re

logger = logging.getLogger(__name__)


class DeepSeekLocalClient:
    """DeepSeekæœ¬åœ°éƒ¨ç½²å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        model_path: str = "./models/deepseek/deepseek-coder-6.7b",
        device: str = "cuda",
        load_in_8bit: bool = False,
        load_in_4bit: bool = False
    ):
        """
        åˆå§‹åŒ–æœ¬åœ°DeepSeekå®¢æˆ·ç«¯
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
            load_in_8bit: æ˜¯å¦ä½¿ç”¨8ä½é‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
            load_in_4bit: æ˜¯å¦ä½¿ç”¨4ä½é‡åŒ–ï¼ˆæ›´èŠ‚çœæ˜¾å­˜ï¼‰
        """
        self.model_path = model_path
        self.device = device if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ğŸš€ åŠ è½½DeepSeekæœ¬åœ°æ¨¡å‹: {model_path}")
        
        # åŠ è½½tokenizer
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
        except Exception as e:
            logger.error(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
            raise
        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        try:
            if load_in_4bit or load_in_8bit:
                from transformers import BitsAndBytesConfig
                
                if load_in_4bit:
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16
                    )
                else:
                    quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    quantization_config=quantization_config,
                    device_map="auto" if self.device == "cuda" else None
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    device_map="auto" if self.device == "cuda" else None
                )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            self.model.eval()
            logger.info("âœ… DeepSeekæœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
        }
    
    def analyze_trading_signal(
        self,
        market_data: Dict[str, Any],
        features: Dict[str, float],
        sentiment_data: Optional[Dict[str, Any]] = None,
        on_chain_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        åˆ†æäº¤æ˜“ä¿¡å·
        
        Args:
            market_data: å¸‚åœºæ•°æ®
            features: æŠ€æœ¯æŒ‡æ ‡
            sentiment_data: æƒ…ç»ªæ•°æ®
            on_chain_data: é“¾ä¸Šæ•°æ®
            
        Returns:
            äº¤æ˜“ä¿¡å·å­—å…¸
        """
        self.stats["total_requests"] += 1
        
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_analysis_prompt(
                market_data, features, sentiment_data, on_chain_data
            )
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.95,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”
            response_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            # è§£æå“åº”
            result = self._parse_response(response_text)
            
            self.stats["successful_requests"] += 1
            logger.info(
                f"DeepSeekæœ¬åœ°åˆ†æå®Œæˆ: signal={result['signal']}, "
                f"confidence={result['confidence']:.2f}"
            )
            
            return result
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"DeepSeekæœ¬åœ°æ¨ç†å¤±è´¥: {e}")
            raise
    
    def _build_analysis_prompt(
        self,
        market_data: Dict[str, Any],
        features: Dict[str, float],
        sentiment_data: Optional[Dict[str, Any]],
        on_chain_data: Optional[Dict[str, Any]]
    ) -> str:
        """
        æ„å»ºåˆ†ææç¤ºè¯
        
        å‡½æ•°çº§æ³¨é‡Šï¼šå°†å¸‚åœºæ•°æ®ã€æŠ€æœ¯æŒ‡æ ‡ã€æƒ…ç»ªå’Œé“¾ä¸Šæ•°æ®ç»„åˆæˆç»“æ„åŒ–çš„æç¤ºè¯
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“AIåŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹æ•°æ®ï¼Œç»™å‡ºäº¤æ˜“å»ºè®®ã€‚

## å¸‚åœºæ•°æ®
- äº¤æ˜“å¯¹: {market_data.get('symbol', 'UNKNOWN')}
- å½“å‰ä»·æ ¼: ${market_data.get('current_price', market_data.get('price', 0)):.2f}
- 24hæ¶¨è·Œ: {market_data.get('change_24h', 0):.2f}%
- 24hæˆäº¤é‡: ${market_data.get('volume_24h', market_data.get('volume', 0)):,.0f}
- 24hæœ€é«˜: ${market_data.get('high_24h', 0):.2f}
- 24hæœ€ä½: ${market_data.get('low_24h', 0):.2f}

## æŠ€æœ¯æŒ‡æ ‡
"""
        
        # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
        for key, value in features.items():
            if isinstance(value, (int, float)):
                prompt += f"- {key}: {value:.2f}\n"
        
        # æ·»åŠ æƒ…ç»ªæ•°æ®
        if sentiment_data:
            prompt += f"\n## å¸‚åœºæƒ…ç»ª\n"
            prompt += f"- ææƒ§è´ªå©ªæŒ‡æ•°: {sentiment_data.get('fear_greed_index', 50)}\n"
            prompt += f"- ç¤¾äº¤åª’ä½“æƒ…ç»ª: {sentiment_data.get('social_sentiment', 'neutral')}\n"
        
        # æ·»åŠ é“¾ä¸Šæ•°æ®
        if on_chain_data:
            prompt += f"\n## é“¾ä¸Šæ•°æ®\n"
            prompt += f"- äº¤æ˜“æ‰€æµå…¥: ${on_chain_data.get('exchange_inflow', 0):,.0f}\n"
            prompt += f"- äº¤æ˜“æ‰€æµå‡º: ${on_chain_data.get('exchange_outflow', 0):,.0f}\n"
            prompt += f"- æ´»è·ƒåœ°å€æ•°: {on_chain_data.get('active_addresses', 0):,}\n"
        
        prompt += """
## ä»»åŠ¡è¦æ±‚
è¯·ç»¼åˆåˆ†æä»¥ä¸Šæ‰€æœ‰æ•°æ®ï¼Œç»™å‡ºäº¤æ˜“å»ºè®®ã€‚æ³¨æ„ï¼š
1. è€ƒè™‘å¤šä¸ªæ—¶é—´å‘¨æœŸï¼ˆçŸ­æœŸã€ä¸­æœŸã€é•¿æœŸï¼‰
2. è¯†åˆ«å…³é”®æ”¯æ’‘ä½å’Œé˜»åŠ›ä½
3. è¯„ä¼°å¸‚åœºæƒ…ç»ªå’Œèµ„é‡‘æµå‘
4. ç»™å‡ºæ¸…æ™°çš„é£é™©æ§åˆ¶å»ºè®®

## è¾“å‡ºæ ¼å¼
è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
    "signal": "BUY" æˆ– "SELL" æˆ– "HOLD",
    "confidence": 0.0åˆ°1.0çš„æ•°å­—ï¼Œè¡¨ç¤ºä¿¡å·ç½®ä¿¡åº¦,
    "position_size": 0.0åˆ°1.0çš„æ•°å­—ï¼Œå»ºè®®å¼€ä»“çš„èµ„é‡‘æ¯”ä¾‹,
    "stop_loss": æ­¢æŸä»·æ ¼ï¼ˆæ•°å­—ï¼‰,
    "take_profit": æ­¢ç›ˆä»·æ ¼ï¼ˆæ•°å­—ï¼‰,
    "reasoning": "è¯¦ç»†çš„åˆ†æç†ç”±ï¼ŒåŒ…æ‹¬æŠ€æœ¯é¢ã€æƒ…ç»ªé¢ã€èµ„é‡‘é¢çš„ç»¼åˆåˆ¤æ–­"
}

è¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚
"""
        return prompt
    
    def _parse_response(self, response_text: str) -> Dict[str, Any]:
        """
        è§£æå“åº”
        
        å‡½æ•°çº§æ³¨é‡Šï¼šä»æ¨¡å‹å“åº”ä¸­æå–JSONæ ¼å¼çš„äº¤æ˜“ä¿¡å·
        """
        # å°è¯•æå–JSON
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            try:
                result = json.loads(json_match.group())
                
                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = [
                    "signal", "confidence", "position_size", 
                    "stop_loss", "take_profit", "reasoning"
                ]
                
                for field in required_fields:
                    if field not in result:
                        if field in ["stop_loss", "take_profit"]:
                            result[field] = None
                        else:
                            raise ValueError(f"å“åº”ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                
                # éªŒè¯ä¿¡å·å€¼
                if result["signal"] not in ["BUY", "SELL", "HOLD"]:
                    logger.warning(f"æ— æ•ˆçš„ä¿¡å·å€¼: {result['signal']}ï¼Œä½¿ç”¨HOLD")
                    result["signal"] = "HOLD"
                
                # éªŒè¯æ•°å€¼èŒƒå›´
                if not (0 <= result["confidence"] <= 1):
                    result["confidence"] = max(0.0, min(1.0, result["confidence"]))
                
                if not (0 <= result["position_size"] <= 1):
                    result["position_size"] = max(0.0, min(1.0, result["position_size"]))
                
                return result
            except json.JSONDecodeError as e:
                logger.warning(f"JSONè§£æå¤±è´¥: {e}")
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        logger.warning("æ— æ³•è§£æå“åº”ï¼Œè¿”å›é»˜è®¤HOLDä¿¡å·")
        return {
            "signal": "HOLD",
            "confidence": 0.5,
            "position_size": 0.0,
            "stop_loss": None,
            "take_profit": None,
            "reasoning": response_text[:500] if response_text else "æ— æ³•è§£æAIå“åº”"
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–å®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        success_rate = (
            self.stats["successful_requests"] / self.stats["total_requests"] * 100
            if self.stats["total_requests"] > 0
            else 0
        )
        
        return {
            **self.stats,
            "success_rate": success_rate,
            "model_path": self.model_path,
            "device": self.device
        }
    
    def close(self):
        """å…³é—­å®¢æˆ·ç«¯ï¼Œé‡Šæ”¾èµ„æº"""
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logger.info("DeepSeekæœ¬åœ°å®¢æˆ·ç«¯å·²å…³é—­")

